{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBXbFax4YP34"
   },
   "source": [
    "## Lab: MNIST Autoencoder\n",
    "\n",
    "You will now work on an autoencoder that works on the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist). This will encode the inputs to lower resolution images. The decoder should then be able to generate the original input from this compressed representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (4.9.6)\n",
      "Requirement already satisfied: absl-py in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (4.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (14.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (2.32.2)\n",
      "Requirement already satisfied: simple-parsing in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.9.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.4)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.11.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\joselyn\\downloads\\anaconda\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZYaLxnBYUKA"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3EXwoz-KHtWO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is05FyRgYX0c"
   },
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xuhe2ksPI8A0"
   },
   "source": [
    "You will load the MNIST data from TFDS into train and test sets. Let's first define a preprocessing function for normalizing and flattening the images. Since we'll be training an autoencoder, this will return `image, image` because the input will also be the target or label while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t9F7YsCNIKSA"
   },
   "outputs": [],
   "source": [
    "def map_image(image, label):\n",
    "  '''Normalizes and flattens the image. Returns image as input and label.'''\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = image / 255.0\n",
    "  image = tf.reshape(image, shape=(784,))\n",
    "\n",
    "  return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "08I1mbYSLbsC"
   },
   "outputs": [],
   "source": [
    "# Load the train and test sets from TFDS\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "\n",
    "train_dataset = tfds.load('mnist', as_supervised=True, split=\"train\")\n",
    "train_dataset = train_dataset.map(map_image)\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_dataset = tfds.load('mnist', as_supervised=True, split=\"test\")\n",
    "test_dataset = test_dataset.map(map_image)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z70w2KXjYk32"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MKu48lBM2Qg"
   },
   "source": [
    "You will now build a simple autoencoder to ingest the data. Like before, the encoder will compress the input and reconstructs it in the decoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KRrE2BV4IpzR"
   },
   "outputs": [],
   "source": [
    "def simple_autoencoder(inputs):\n",
    "  '''Builds the encoder and decoder using Dense layers.'''\n",
    "  # START YOUR CODE HERE\n",
    "  encoder = tf.keras.layers.Dense(units=32,activation='relu')(inputs)\n",
    "  decoder = tf.keras.layers.Dense(units=784,activation=\"sigmoid\")(encoder)\n",
    "  # END YOUR CODE HERE\n",
    "  return encoder, decoder\n",
    "\n",
    "# set the input shape\n",
    "inputs =  tf.keras.layers.Input(shape=(784,))\n",
    "\n",
    "# get the encoder and decoder output\n",
    "encoder_output, decoder_output = simple_autoencoder(inputs)\n",
    "\n",
    "# setup the encoder because you will visualize its output later\n",
    "encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_output)\n",
    "\n",
    "# setup the autoencoder\n",
    "autoencoder_model = tf.keras.Model(inputs=inputs, outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k9OnSM4YxJd"
   },
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvvDqY_XQPyb"
   },
   "source": [
    "You will setup the model for training. You can use binary crossentropy to measure the loss between pixel values that range from 0 (black) to 1 (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cFwmAhWAYwcc"
   },
   "outputs": [],
   "source": [
    "autoencoder_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzLf0oQ1Y0cI"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vsaSjlAgYz-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.3163\n",
      "Epoch 2/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1504\n",
      "Epoch 3/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1230\n",
      "Epoch 4/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1102\n",
      "Epoch 5/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1026\n",
      "Epoch 6/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0984\n",
      "Epoch 7/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0960\n",
      "Epoch 8/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0948\n",
      "Epoch 9/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0941\n",
      "Epoch 10/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0937\n",
      "Epoch 11/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0937\n",
      "Epoch 12/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0935\n",
      "Epoch 13/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0933\n",
      "Epoch 14/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0932\n",
      "Epoch 15/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0932\n",
      "Epoch 16/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0932\n",
      "Epoch 17/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0931\n",
      "Epoch 18/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0930\n",
      "Epoch 19/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0930\n",
      "Epoch 20/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0930\n",
      "Epoch 21/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0930\n",
      "Epoch 22/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0929\n",
      "Epoch 23/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0929\n",
      "Epoch 24/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0930\n",
      "Epoch 25/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0929\n",
      "Epoch 26/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 27/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0929\n",
      "Epoch 28/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0928\n",
      "Epoch 29/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0928\n",
      "Epoch 30/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 31/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 32/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 33/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 34/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 35/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 36/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 37/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 38/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 39/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 40/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 41/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 42/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 43/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 44/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0926\n",
      "Epoch 45/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 46/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 47/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0926\n",
      "Epoch 48/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0928\n",
      "Epoch 49/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0927\n",
      "Epoch 50/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0928\n"
     ]
    }
   ],
   "source": [
    "train_steps = 60000 // BATCH_SIZE\n",
    "simple_auto_history = autoencoder_model.fit(train_dataset, steps_per_epoch=train_steps, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_sKLXnBZFSc"
   },
   "source": [
    "## Display sample results\n",
    "\n",
    "You can now visualize the results. The utility functions below will help in plotting the encoded and decoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5tgFgilORr0M"
   },
   "outputs": [],
   "source": [
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Display sample outputs in one row.'''\n",
    "  for idx, test_image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    test_image = np.reshape(test_image, shape)\n",
    "    plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
    "  '''Displays the input, encoded, and decoded output values.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
    "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
    "  display_one_row(disp_predicted, 20, shape=(28,28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtQyQRxRN_hH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n"
     ]
    }
   ],
   "source": [
    "# take 1 batch of the dataset\n",
    "test_dataset = test_dataset.take(1)\n",
    "\n",
    "# take the input images and put them in a list\n",
    "output_samples = []\n",
    "for input_image, image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "# pick 10 random numbers to be used as indices to the list above\n",
    "idxs = np.random.choice(BATCH_SIZE, size=10)\n",
    "\n",
    "# get the encoder output\n",
    "encoded_predicted = encoder_model.predict(test_dataset)\n",
    "\n",
    "# get a prediction for the test batch\n",
    "simple_predicted = autoencoder_model.predict(test_dataset)\n",
    "\n",
    "# display the 10 samples, encodings and decoded values!\n",
    "display_results(output_samples[idxs], encoded_predicted[idxs], simple_predicted[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
