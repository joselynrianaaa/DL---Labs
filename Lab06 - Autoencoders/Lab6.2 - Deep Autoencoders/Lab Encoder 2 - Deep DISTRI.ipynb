{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSmjE4t1dKnG"
   },
   "source": [
    "# Lab: MNIST Deep Autoencoder\n",
    "\n",
    "Welcome back! In this lab, you will extend the shallow autoencoder you built in the previous exercise. The model here will have a deeper network so it can handle more complex images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTTfAJbudq7l"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3EXwoz-KHtWO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25T24ET7e0ub"
   },
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RJB4eae0rgU"
   },
   "source": [
    "You will prepare the MNIST dataset just like in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5BMJF86p0d08"
   },
   "outputs": [],
   "source": [
    "def map_image(image, label):\n",
    "  '''Normalizes and flattens the image. Returns image as input and label.'''\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = image / 255.0\n",
    "  image = tf.reshape(image, shape=(784,))\n",
    "\n",
    "  return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t9F7YsCNIKSA"
   },
   "outputs": [],
   "source": [
    "# Load the train and test sets from TFDS\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "\n",
    "train_dataset = tfds.load('mnist', as_supervised=True, split=\"train\")\n",
    "train_dataset = train_dataset.map(map_image)\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_dataset = tfds.load('mnist', as_supervised=True, split=\"test\")\n",
    "test_dataset = test_dataset.map(map_image)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji6sa0SXe3zP"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8OFTvkO02gV"
   },
   "source": [
    "As mentioned, you will have a deeper network for the autoencoder. Compare the layers here with that of the shallow network you built in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KRrE2BV4IpzR"
   },
   "outputs": [],
   "source": [
    "def deep_autoencoder():\n",
    "    '''Builds the encoder and decoder using Dense layers.'''\n",
    "    # START YOUR CODE HERE\n",
    "    encoder = tf.keras.layers.Dense(units=128, activation='relu')(inputs)\n",
    "    encoder = tf.keras.layers.Dense(units=64, activation='relu')(encoder)\n",
    "    encoder = tf.keras.layers.Dense(units=32, activation='relu')(encoder)\n",
    "    \n",
    "    # Decoder layers\n",
    "    decoder = tf.keras.layers.Dense(units=64, activation='relu')(encoder)\n",
    "    decoder = tf.keras.layers.Dense(units=128, activation='relu')(decoder)\n",
    "    decoder = tf.keras.layers.Dense(units=784, activation='sigmoid')(decoder)\n",
    "    # END YOUR CODE HERE\n",
    "    return encoder, decoder\n",
    "\n",
    "# set the input tensor\n",
    "inputs = tf.keras.layers.Input(shape=(784,))\n",
    "\n",
    "# get the encoder and decoder output\n",
    "deep_encoder_output, deep_autoencoder_output = deep_autoencoder()\n",
    "\n",
    "# setup the encoder because you will visualize its output later\n",
    "deep_encoder_model = tf.keras.Model(inputs=inputs, outputs=deep_encoder_output)\n",
    "\n",
    "# setup the autoencoder\n",
    "deep_autoencoder_model = tf.keras.Model(inputs=inputs, outputs=deep_autoencoder_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zG5ml6ufAz7"
   },
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gj3xr9gxfAqP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.2815\n",
      "Epoch 2/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.1359\n",
      "Epoch 3/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.1192\n",
      "Epoch 4/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.1123\n",
      "Epoch 5/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.1076\n",
      "Epoch 6/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.1041\n",
      "Epoch 7/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.1014\n",
      "Epoch 8/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0989\n",
      "Epoch 9/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0971\n",
      "Epoch 10/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0956\n",
      "Epoch 11/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0941\n",
      "Epoch 12/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0932\n",
      "Epoch 13/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0923\n",
      "Epoch 14/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0914\n",
      "Epoch 15/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0905\n",
      "Epoch 16/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0897\n",
      "Epoch 17/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0890\n",
      "Epoch 18/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0883\n",
      "Epoch 19/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0879\n",
      "Epoch 20/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0875\n",
      "Epoch 21/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0871\n",
      "Epoch 22/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0866\n",
      "Epoch 23/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0863\n",
      "Epoch 24/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0859\n",
      "Epoch 25/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0856\n",
      "Epoch 26/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0854\n",
      "Epoch 27/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0850\n",
      "Epoch 28/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0849\n",
      "Epoch 29/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0847\n",
      "Epoch 30/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0845\n",
      "Epoch 31/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0843\n",
      "Epoch 32/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0842\n",
      "Epoch 33/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0841\n",
      "Epoch 34/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0840\n",
      "Epoch 35/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0838\n",
      "Epoch 36/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0837\n",
      "Epoch 37/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0837\n",
      "Epoch 38/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0835\n",
      "Epoch 39/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0834\n",
      "Epoch 40/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0833\n",
      "Epoch 41/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0832\n",
      "Epoch 42/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0832\n",
      "Epoch 43/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0830\n",
      "Epoch 44/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0830\n",
      "Epoch 45/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0828\n",
      "Epoch 46/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0828\n",
      "Epoch 47/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0827\n",
      "Epoch 48/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0826\n",
      "Epoch 49/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0827\n",
      "Epoch 50/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0825\n"
     ]
    }
   ],
   "source": [
    "train_steps = 60000 // BATCH_SIZE\n",
    "\n",
    "deep_autoencoder_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
    "deep_auto_history = deep_autoencoder_model.fit(train_dataset, steps_per_epoch=train_steps, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1XL84ydfGEh"
   },
   "source": [
    "## Display sample results\n",
    "\n",
    "See the results using the model you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GcR54SEZ1-XF"
   },
   "outputs": [],
   "source": [
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Display sample outputs in one row.'''\n",
    "  for idx, test_image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    test_image = np.reshape(test_image, shape)\n",
    "    plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
    "  '''Displays the input, encoded, and decoded output values.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
    "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
    "  display_one_row(disp_predicted, 20, shape=(28,28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtQyQRxRN_hH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n"
     ]
    }
   ],
   "source": [
    "# take 1 batch of the dataset\n",
    "test_dataset = test_dataset.take(1)\n",
    "\n",
    "# take the input images and put them in a list\n",
    "output_samples = []\n",
    "for input_image, image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "# pick 10 random numbers to be used as indices to the list above\n",
    "idxs = np.random.choice(BATCH_SIZE, size=10)\n",
    "\n",
    "# get the encoder output\n",
    "encoded_predicted = deep_encoder_model.predict(test_dataset)\n",
    "\n",
    "# get a prediction for the test batch\n",
    "deep_predicted = deep_autoencoder_model.predict(test_dataset)\n",
    "\n",
    "# display the 10 samples, encodings and decoded values!\n",
    "display_results(output_samples[idxs], encoded_predicted[idxs], deep_predicted[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
